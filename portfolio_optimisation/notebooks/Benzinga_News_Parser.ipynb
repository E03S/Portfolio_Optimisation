{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'BENZINGA_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/sprilut/workspace/hse/Portfolio_Optimisation/NewsParser.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sprilut/workspace/hse/Portfolio_Optimisation/NewsParser.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sprilut/workspace/hse/Portfolio_Optimisation/NewsParser.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mzipfile\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sprilut/workspace/hse/Portfolio_Optimisation/NewsParser.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m api_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49menviron[\u001b[39m'\u001b[39;49m\u001b[39mBENZINGA_API_KEY\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sprilut/workspace/hse/Portfolio_Optimisation/NewsParser.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(api_key)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sprilut/workspace/hse/Portfolio_Optimisation/NewsParser.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremove_html_tags\u001b[39m(text):\n",
      "File \u001b[0;32m<frozen os>:685\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'BENZINGA_API_KEY'"
     ]
    }
   ],
   "source": [
    "from benzinga import news_data\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
    "import concurrent.futures\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning, module='bs4')\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "api_key = 'YOUR_API_KEY'\n",
    "print(api_key)\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "date_from = \"2007-01-01\"\n",
    "paper = news_data.News(api_key, log=False)\n",
    "def get_news(ticker, page, date_from, date_to, display_output=\"full\"):\n",
    "    news = paper.news(company_tickers=ticker, display_output=display_output, date_from=date_from, date_to=date_to, page=page, pagesize=100)\n",
    "    if (len(news) == 0):\n",
    "        return []\n",
    "    df = pd.DataFrame(news)\n",
    "    df['teaser'] = df['teaser'].apply(remove_html_tags)\n",
    "    df['body'] = df['body'].apply(remove_html_tags)\n",
    "    return df\n",
    "\n",
    "tickers = [\n",
    "    'MSFT',\n",
    "    'AMZN',\n",
    "    'NVDA',\n",
    "    'AAPL',\n",
    "    'GOOGL',\n",
    "    'META',\n",
    "    'GOOG',\n",
    "    'BRK.B',\n",
    "    'TSLA',\n",
    "    'UNH',\n",
    "    'LLY',\n",
    "    'XOM',\n",
    "    'JPM',\n",
    "    'V',\n",
    "    'JNJ',\n",
    "    'PG',\n",
    "    'AVGO',\n",
    "    'MA',\n",
    "    'HD',\n",
    "    'CVX',\n",
    "    'MRK',\n",
    "    'ABBV',\n",
    "    'COST',\n",
    "    'ADBE',\n",
    "    'WMT',\n",
    "    'PEP',\n",
    "    'KO',\n",
    "    'CSCO',\n",
    "    'CRM',\n",
    "    'MCD',\n",
    "    'ACN',\n",
    "    'LIN',\n",
    "    'BAC',\n",
    "    'NFLX',\n",
    "    'PFE',\n",
    "    'TMO',\n",
    "    'ABT',\n",
    "    'CMCSA',\n",
    "    'ORCL',\n",
    "    'AMD',\n",
    "    'DIS',\n",
    "    'WFC',\n",
    "    'VZ',\n",
    "    'AMGN',\n",
    "    'COP',\n",
    "    'PM',\n",
    "    'INTC',\n",
    "    'INTU',\n",
    "    'IBM',\n",
    "    'TXN',\n",
    "    'DHR',\n",
    "    'CAT',\n",
    "    'UNP',\n",
    "    'NKE',\n",
    "    'GE',\n",
    "    'QCOM',\n",
    "    'HON',\n",
    "    'NEE',\n",
    "    'RTX',\n",
    "    'SPGI',\n",
    "    'NOW',\n",
    "    'BMY',\n",
    "    'AMAT',\n",
    "    'LOW',\n",
    "    'T',\n",
    "    'SBUX',\n",
    "    'ELV',\n",
    "    'BA',\n",
    "    'TJX',\n",
    "    'DE',\n",
    "    'UPS',\n",
    "    'LMT',\n",
    "    'GS',\n",
    "    'BKNG',\n",
    "    'GILD',\n",
    "    'MDT',\n",
    "    'VRTX',\n",
    "    'MMC',\n",
    "    'PLD',\n",
    "    'MS',\n",
    "    'ISRG',\n",
    "    'ADP',\n",
    "    'PGR',\n",
    "    'CI',\n",
    "    'MDLZ',\n",
    "    'CB',\n",
    "    'SYK',\n",
    "    'CVS',\n",
    "    'BLK',\n",
    "    'REGN',\n",
    "    'AXP',\n",
    "    'AMT',\n",
    "    'ADI',\n",
    "    'SLB',\n",
    "    'ETN',\n",
    "    'LRCX',\n",
    "    'CME',\n",
    "    'SCHW',\n",
    "    'C',\n",
    "    'EOG'\n",
    "]\n",
    "\n",
    "def create_datasets_folder():\n",
    "    if not os.path.exists('datasets'):\n",
    "        os.makedirs('datasets')\n",
    "\n",
    "def get_news_by_ticker(ticker):\n",
    "    try:\n",
    "        page = 0\n",
    "        main_df = pd.DataFrame()\n",
    "        date_from = \"2007-01-01\"\n",
    "        total = 0\n",
    "        while True:\n",
    "            if page > 100:\n",
    "                date_from  = datetime.strptime(main_df['updated'].iloc[-1], \"%a, %d %b %Y %H:%M:%S %z\").strftime('%Y-%m-%d')\n",
    "                page = 0\n",
    "            news_df = get_news(ticker, page, date_from, today_date, 'full')\n",
    "            if (len(news_df) == 0):\n",
    "                break\n",
    "            main_df = pd.concat([main_df, news_df], ignore_index=True)\n",
    "            main_df = main_df.drop_duplicates(subset=['id'])\n",
    "            page += 1\n",
    "            total += 1\n",
    "            print(f\"{ticker} - {total} page. Added rows: {len(news_df)} total: {len(main_df)}\")\n",
    "        if ticker == 'BRK.B' or ticker == 'BRK.A':\n",
    "            ticker = 'BRK-B'\n",
    "        main_df.to_csv(f\"datasets/news_sp_500_{ticker}.csv\")\n",
    "        return ticker\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def merge_all_in_one_file():\n",
    "    main_df = pd.DataFrame()\n",
    "    for ticker in tickers:\n",
    "        if ticker == 'BRK.B':\n",
    "            ticker = 'BRK-B'\n",
    "        df = pd.read_csv(f\"datasets/news_sp_500_{ticker}.csv\")\n",
    "        main_df = pd.concat([main_df, df], ignore_index=True)\n",
    "        main_df = main_df.drop_duplicates(subset=['id'])\n",
    "    print(f\"Rows in total {len(main_df)}\")\n",
    "    main_df.to_csv(f\"datasets/news_sp_500.csv\")\n",
    "\n",
    "def run_concurent(max_workers=10):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(get_news_by_ticker, ticker) for ticker in tickers]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            ticker = future.result()\n",
    "            print(f\"Ticker {ticker} done\")\n",
    "\n",
    "def check_all_files():\n",
    "    for ticker in tickers:\n",
    "        if ticker == 'BRK.B':\n",
    "            ticker = 'BRK-B'\n",
    "        df = pd.read_csv(f\"datasets/news_sp_500_{ticker}.csv\")\n",
    "        if len(df) == 0:\n",
    "            print(f\"{ticker} - {len(df)}\")\n",
    "\n",
    "def zip_all_datasets():\n",
    "    zf = zipfile.ZipFile('news_datasets.zip', mode='w')\n",
    "    for ticker in tickers:\n",
    "        if ticker == 'BRK.B':\n",
    "            ticker = 'BRK-B'\n",
    "        zf.write(f\"datasets/news_sp_500_{ticker}.csv\")\n",
    "    zf.write(f\"datasets/news_sp_500.csv\")\n",
    "    zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_datasets_folder()\n",
    "\n",
    "# run all tickers in 10 threads\n",
    "run_concurent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all files in case some of them are empty\n",
    "check_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all_in_one_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_all_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "year_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
